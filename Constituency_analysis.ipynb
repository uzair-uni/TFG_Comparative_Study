{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HoJyFWQjfvck",
        "outputId": "26a9527d-ea57-4e45-f9ca-577083aa3b6e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U benepar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sQm3cktFgqHL",
        "outputId": "09dd35a1-e67f-4ba9-a118-80d0ca0e2094"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting benepar\n",
            "  Downloading benepar-0.2.0.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.11/dist-packages (from benepar) (3.9.1)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.11/dist-packages (from benepar) (3.8.7)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from benepar) (2.6.0+cu124)\n",
            "Collecting torch-struct>=0.5 (from benepar)\n",
            "  Downloading torch_struct-0.5-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tokenizers>=0.9.4 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.21.1)\n",
            "Requirement already satisfied: transformers>=4.2.2 in /usr/local/lib/python3.11/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (4.52.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from benepar) (5.29.5)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.0.9->benepar) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.9.4->benepar) (0.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6.0->benepar)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6.0->benepar)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6.0->benepar)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6.0->benepar)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6.0->benepar)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6.0->benepar)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6.0->benepar)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6.0->benepar)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6.0->benepar)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6.0->benepar)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->benepar) (1.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (0.5.3)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (1.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[tokenizers,torch]>=4.2.2->benepar) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.9.4->benepar) (1.1.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2.0.9->benepar) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=2.0.9->benepar) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=2.0.9->benepar) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.0.9->benepar) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.0.9->benepar) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy>=2.0.9->benepar) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.0.9->benepar) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=2.0.9->benepar) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.0.9->benepar) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_struct-0.5-py3-none-any.whl (34 kB)\n",
            "Building wheels for collected packages: benepar\n",
            "  Building wheel for benepar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for benepar: filename=benepar-0.2.0-py3-none-any.whl size=37625 sha256=ca787be24586d6b9f7e0b6bb718d48138ee5057f6e66baad87803d18065b5ba8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/06/97/846995c0825bbc92825ce41675b6d5477213b25e167115223f\n",
            "Successfully built benepar\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch-struct, benepar\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed benepar-0.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-struct-0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3zKXv8zIZeUX",
        "outputId": "96597b10-29b5-4b19-bdb4-1920d5318549"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "from collections import Counter\n",
        "# import shifterator as sh\n",
        "import matplotlib.pyplot as plt\n",
        "import benepar as bnp\n",
        "from benepar.spacy_plugin import BeneparComponent\n",
        "from nltk.tree import Tree\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from datasets import load_from_disk\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nsyp7vKx2rPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24aa81f3-78a6-4c7d-d807-3ea2760272e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/benepar/spacy_plugin.py:7: FutureWarning: BeneparComponent and NonConstituentException have been moved to the benepar module. Use `from benepar import BeneparComponent, NonConstituentException` instead of benepar.spacy_plugin. The benepar.spacy_plugin namespace is deprecated and will be removed in a future version.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzS4l4cZycyq",
        "outputId": "5cf5d95b-e1b1-408c-e8ca-5091e9b5262f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK5wvyyk31IV",
        "outputId": "82705052-a147-484c-8f54-f6f799966f1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# benepar\n",
        "import benepar\n",
        "benepar.download('benepar_en3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQVJ6BjghVTz",
        "outputId": "b425cbf8-7451-4e49-d083-2de711ad6626"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping models/benepar_en3.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spacy model load\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# benepar constituency parser addition (ERRORS SOMETIMES!!)\n",
        "nlp.add_pipe(\"benepar\", config = {'model': 'benepar_en3'})\n"
      ],
      "metadata": {
        "id": "RVhGhJH1uAJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93554d96-c2e6-464e-963a-6937bcb3bd4e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<benepar.integrations.spacy_plugin.BeneparComponent at 0x7ad7279c3e10>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre_folder = '/content/drive/MyDrive/TFG/PRE_ELEC_SCRIPTS'\n",
        "pre_files = [os.path.join(pre_folder, f) for f in os.listdir(pre_folder) if f.endswith('.txt')]\n",
        "\n",
        "post_folder = '/content/drive/MyDrive/TFG/POST_ELEC_SCRIPTS'\n",
        "post_files = [os.path.join(post_folder, f) for f in os.listdir(post_folder) if f.endswith('.txt')]"
      ],
      "metadata": {
        "id": "dnhzG9hU4RwB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constituency parsing (tree)"
      ],
      "metadata": {
        "id": "d9VbJVkYkjPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = {\"my administration\", \"immigrants\", \"islam\", \"hillary clinton\"}\n"
      ],
      "metadata": {
        "id": "X1FAN-uoynf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# New function to extract sentences from a HuggingFace dataset\n",
        "def read_sentences_from_dataset(dataset, text_column=\"sentence\"):\n",
        "    all_sentences = []\n",
        "    for row in dataset:\n",
        "        text = row[text_column].lower()\n",
        "        all_sentences.append(text)\n",
        "    return all_sentences\n",
        "\n",
        "\n",
        "# Reused function (no changes needed)\n",
        "def collect_keyword_sentences(sentences, keywords, max_sentences=100, max_char_len=500):\n",
        "    keyword_sents = defaultdict(list)\n",
        "\n",
        "    for sent in sentences:\n",
        "        sent = sent.strip()\n",
        "        if len(sent) > max_char_len:\n",
        "            continue\n",
        "        for keyword in keywords:\n",
        "            if keyword in sent:\n",
        "                keyword_sents[keyword].append(sent)\n",
        "\n",
        "    for k in keyword_sents:\n",
        "        sents = sorted(keyword_sents[k], key=len, reverse=True)\n",
        "        keyword_sents[k] = sents[:max_sentences]\n",
        "\n",
        "    return keyword_sents\n"
      ],
      "metadata": {
        "id": "kzxhNe_gZaoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find adjectives within the same noun phrase as a keyword\n",
        "def find_adjectives_in_tree(tree, keyword):\n",
        "    adjs = []\n",
        "\n",
        "    for subtree in tree.subtrees(lambda t: t.label() == \"NP\"):\n",
        "        pos_tags = subtree.pos()\n",
        "        phrase_text = \" \".join(w for w, _ in pos_tags).lower()\n",
        "\n",
        "        if keyword in phrase_text:\n",
        "            for word, tag in pos_tags:\n",
        "                if tag.startswith(\"JJ\") or tag in (\"VBG\", \"VBN\"):\n",
        "                    adjs.append(word.lower())\n",
        "\n",
        "    return adjs\n",
        "\n",
        "# Apply to all sentences\n",
        "def extract_adjectives_for_keywords(sentences_by_keyword):\n",
        "    results = defaultdict(list)\n",
        "\n",
        "    for keyword, sentences in sentences_by_keyword.items():\n",
        "        for sentence in sentences:\n",
        "            try:\n",
        "                doc = nlp(sentence)\n",
        "                for sent in doc.sents:\n",
        "                    tree = Tree.fromstring(sent._.parse_string)\n",
        "                    adjs = find_adjectives_in_tree(tree, keyword)\n",
        "                    results[keyword].extend(adjs)\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping sentence due to error: {e}\")\n",
        "                continue\n",
        "\n",
        "    # deduplication, can be skipped to get adjective frequency if needed\n",
        "    for k in results:\n",
        "        results[k] = list(set(results[k]))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "def get_example_sentences_per_keyword(sentences_by_keyword, nlp, max_examples=15):\n",
        "    examples = defaultdict(list)\n",
        "\n",
        "    for keyword, sentences in sentences_by_keyword.items():\n",
        "        count = 0\n",
        "        for sentence in sentences:\n",
        "            if count >= max_examples:\n",
        "                break\n",
        "            try:\n",
        "                doc = nlp(sentence)\n",
        "                for sent in doc.sents:\n",
        "                    tree = Tree.fromstring(sent._.parse_string)\n",
        "                    # Search for NP containing the keyword\n",
        "                    for subtree in tree.subtrees(lambda t: t.label() == \"NP\"):\n",
        "                        phrase_text = \" \".join(w for w, _ in subtree.pos()).lower()\n",
        "                        if keyword in phrase_text:\n",
        "                            examples[keyword].append(sentence)\n",
        "                            count += 1\n",
        "                            break  # Move to next sentence after match\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping sentence due to error: {e}\")\n",
        "                continue\n",
        "\n",
        "    return examples"
      ],
      "metadata": {
        "id": "F9HYPWAouP2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "pre_dataset = load_from_disk(\"/content/drive/MyDrive/TFG/sent_datasets/pre_dataset_cache\")\n",
        "post_dataset = load_from_disk(\"/content/drive/MyDrive/TFG/sent_datasets/post_dataset_cache\")\n",
        "\n",
        "\n",
        "pre_sentences = read_sentences_from_dataset(pre_dataset)\n",
        "post_sentences = read_sentences_from_dataset(post_dataset)"
      ],
      "metadata": {
        "id": "K6AX9kNjZuQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keyword sentence extraction\n",
        "pre_results = collect_keyword_sentences(pre_sentences, keywords)\n",
        "post_results = collect_keyword_sentences(post_sentences, keywords)"
      ],
      "metadata": {
        "id": "ir_eOpN2aUV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjective extraction\n",
        "pre_adjs = extract_adjectives_for_keywords(pre_results)\n",
        "post_adjs = extract_adjectives_for_keywords(post_results)\n",
        "\n"
      ],
      "metadata": {
        "id": "92mGALp_uqPL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fbb95f6-aa49-47c1-bd10-e78fa925ff48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/distributions/distribution.py:56: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exemplary sentences\n",
        "sents_pre = get_example_sentences_per_keyword(pre_results, nlp)\n",
        "sents_post = get_example_sentences_per_keyword(post_results, nlp)"
      ],
      "metadata": {
        "id": "GC_d67jvDK--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pre_adjs['immigrants'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmiIwisEuu6V",
        "outputId": "1a2a469a-700a-4d1f-dc50-eb52fb9da7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post_adjs['immigrants']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjSI7iuDTBmo",
        "outputId": "291d911e-c670-4c41-ea85-790c5541d1d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['illegal',\n",
              " 'united',\n",
              " 'tremendous',\n",
              " 'going',\n",
              " 'made',\n",
              " 'non',\n",
              " 'irish',\n",
              " 'multiple']"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sents_post['immigrants']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM6IkBfJI5lh",
        "outputId": "7369d75e-aa9e-4036-cb85-f8b6f87268b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we had one instance in australia — i have a lot of respect for australia, i love australia as a country — but we had a problem where, for whatever reason, president obama said that they were going to take probably well over a thousand illegal immigrants who were in prisons and they were going to bring them and take them into this country.',\n",
              " 'we’re here today to celebrate america’s commitment to ireland and the tremendous contributions — and i know it well — the irish immigrants and their descendants have made right here in the united states and throughout the world.',\n",
              " 'they’ve had members of their family killed by illegal immigrants and, really, people with multiple — in some cases, multiple deportations.',\n",
              " 'suspend the entry of all aliens or any class of aliens as immigrants or non-immigrants, or impose on the entry of aliens.']"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extras"
      ],
      "metadata": {
        "id": "9A3_CawCs9lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# not necessary if add_pipe above works properly\n",
        "\n",
        "# if not nlp.has_pipe(\"benepar\"):\n",
        "#     try:\n",
        "#         nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en2\"})\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error loading benepar model: {e}\")"
      ],
      "metadata": {
        "id": "gN8UM5egxbre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tree import Tree\n",
        "import traceback\n",
        "\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "# nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "def find_adjectives_in_tree(tree, keywords):\n",
        "    keyword_adjs = defaultdict(list)\n",
        "\n",
        "    for subtree in tree.subtrees(lambda t: t.label() == \"NP\"):\n",
        "        pos_tags = subtree.pos()\n",
        "\n",
        "        # Build noun phrase and adjective list\n",
        "        phrase_words = [w for w, _ in pos_tags]\n",
        "        phrase = \" \".join(phrase_words).lower()\n",
        "\n",
        "        adjs = [w for w, t in pos_tags if t.startswith(\"JJ\") or t in (\"VBN\", \"VBG\")]\n",
        "\n",
        "        # Match full phrase or any part of it\n",
        "        for keyword in keywords:\n",
        "            if keyword in phrase:\n",
        "                keyword_adjs[keyword].extend([adj.lower() for adj in adjs])\n",
        "\n",
        "    return keyword_adjs\n",
        "\n",
        "\n",
        "def parse_and_extract_adjs(doc, keywords):\n",
        "    results = defaultdict(list)\n",
        "    for s in doc.sents:\n",
        "        try:\n",
        "            tree = Tree.fromstring(s._.parse_string)\n",
        "            adjs = find_adjectives_in_tree(tree, keywords)\n",
        "            for k, v in adjs.items():\n",
        "                results[k].extend(v)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping sentence due to error: {e}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "def process_all_files(file_list, keywords):\n",
        "    all_results = defaultdict(list)\n",
        "\n",
        "    for file_path in file_list:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read().lower()\n",
        "            try:\n",
        "                doc = nlp(text)\n",
        "            except Exception as e:\n",
        "                print(f\"\\n❌ Error in file: {file_path}\")\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "            file_results = parse_and_extract_adjs(doc, keywords)\n",
        "\n",
        "            for k, v in file_results.items():\n",
        "                all_results[k].extend(v)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uS3GEMCgsQwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = {\"united states\", \"hillary clinton\", \"america\", \"our country\", \"my opponent\", \"barack obama\", \"our campaign\", \"donald trump\", \"your jobs\", \"our jobs\", \"illegal immigration\", \"immigration\"}  # customize as needed\n",
        "\n",
        "results = process_all_files(pre_files, keywords)\n",
        "\n",
        "for k in results:\n",
        "    results[k] = list(set(results[k]))\n",
        "\n",
        "# output\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l7wtnPB5tqAz",
        "outputId": "a50985ea-90e0-4afc-a6e9-005d2c5f9e00",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Error in file: /content/drive/MyDrive/TFG/PRE_ELEC_SCRIPTS/Remarks_Announcing_Candidacy_for_President_in_New_York_City.txt\n",
            "\n",
            "❌ Error in file: /content/drive/MyDrive/TFG/PRE_ELEC_SCRIPTS/Statement_by_Donald_J__Trump_on_Veterans_Day.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 74, in retokenize\n",
            "    token_idx, (token_start, token_end) = next(offset_mapping_iter)\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "StopIteration\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-26-11fd102db1c0>\", line 48, in process_all_files\n",
            "    doc = nlp(text)\n",
            "          ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1057, in __call__\n",
            "    error_handler(name, proc, [doc], e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/util.py\", line 1722, in raise_error\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1052, in __call__\n",
            "    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/integrations/spacy_plugin.py\", line 151, in __call__\n",
            "    self._parser.parse(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in parse\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in <listcomp>\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 193, in encode\n",
            "    encoded = self.retokenizer(example.words, example.space_after)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 150, in __call__\n",
            "    example = retokenize(self.tokenizer, words, space_after, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 76, in retokenize\n",
            "    assert word_idx == len(words) - 1\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 74, in retokenize\n",
            "    token_idx, (token_start, token_end) = next(offset_mapping_iter)\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "StopIteration\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-26-11fd102db1c0>\", line 48, in process_all_files\n",
            "    doc = nlp(text)\n",
            "          ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1057, in __call__\n",
            "    error_handler(name, proc, [doc], e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/util.py\", line 1722, in raise_error\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1052, in __call__\n",
            "    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/integrations/spacy_plugin.py\", line 151, in __call__\n",
            "    self._parser.parse(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in parse\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in <listcomp>\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 193, in encode\n",
            "    encoded = self.retokenizer(example.words, example.space_after)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 150, in __call__\n",
            "    example = retokenize(self.tokenizer, words, space_after, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 76, in retokenize\n",
            "    assert word_idx == len(words) - 1\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Error in file: /content/drive/MyDrive/TFG/PRE_ELEC_SCRIPTS/Statement_by_Donald_J__Trump_Responding_to_the_Lies_of_Senator_Cruz_and_Warns_of_Legal_Action.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 74, in retokenize\n",
            "    token_idx, (token_start, token_end) = next(offset_mapping_iter)\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "StopIteration\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-26-11fd102db1c0>\", line 48, in process_all_files\n",
            "    doc = nlp(text)\n",
            "          ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1057, in __call__\n",
            "    error_handler(name, proc, [doc], e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/util.py\", line 1722, in raise_error\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1052, in __call__\n",
            "    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/integrations/spacy_plugin.py\", line 151, in __call__\n",
            "    self._parser.parse(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in parse\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in <listcomp>\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 193, in encode\n",
            "    encoded = self.retokenizer(example.words, example.space_after)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 150, in __call__\n",
            "    example = retokenize(self.tokenizer, words, space_after, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 76, in retokenize\n",
            "    assert word_idx == len(words) - 1\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Error in file: /content/drive/MyDrive/TFG/PRE_ELEC_SCRIPTS/Remarks_at_the_AIPAC_Policy_Conference_in_Washington__DC.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 74, in retokenize\n",
            "    token_idx, (token_start, token_end) = next(offset_mapping_iter)\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "StopIteration\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-26-11fd102db1c0>\", line 48, in process_all_files\n",
            "    doc = nlp(text)\n",
            "          ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1057, in __call__\n",
            "    error_handler(name, proc, [doc], e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/util.py\", line 1722, in raise_error\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1052, in __call__\n",
            "    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/integrations/spacy_plugin.py\", line 151, in __call__\n",
            "    self._parser.parse(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in parse\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in <listcomp>\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 193, in encode\n",
            "    encoded = self.retokenizer(example.words, example.space_after)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 150, in __call__\n",
            "    example = retokenize(self.tokenizer, words, space_after, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 76, in retokenize\n",
            "    assert word_idx == len(words) - 1\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Error in file: /content/drive/MyDrive/TFG/PRE_ELEC_SCRIPTS/Remarks_on_Foreign_Policy.txt\n",
            "\n",
            "❌ Error in file: /content/drive/MyDrive/TFG/PRE_ELEC_SCRIPTS/Statement_by_Donald_J__Trump_Regarding_Trump_University.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 74, in retokenize\n",
            "    token_idx, (token_start, token_end) = next(offset_mapping_iter)\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "StopIteration\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-26-11fd102db1c0>\", line 48, in process_all_files\n",
            "    doc = nlp(text)\n",
            "          ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1057, in __call__\n",
            "    error_handler(name, proc, [doc], e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/util.py\", line 1722, in raise_error\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1052, in __call__\n",
            "    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/integrations/spacy_plugin.py\", line 151, in __call__\n",
            "    self._parser.parse(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in parse\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in <listcomp>\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 193, in encode\n",
            "    encoded = self.retokenizer(example.words, example.space_after)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 150, in __call__\n",
            "    example = retokenize(self.tokenizer, words, space_after, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 76, in retokenize\n",
            "    assert word_idx == len(words) - 1\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 74, in retokenize\n",
            "    token_idx, (token_start, token_end) = next(offset_mapping_iter)\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "StopIteration\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-26-11fd102db1c0>\", line 48, in process_all_files\n",
            "    doc = nlp(text)\n",
            "          ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1057, in __call__\n",
            "    error_handler(name, proc, [doc], e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/util.py\", line 1722, in raise_error\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1052, in __call__\n",
            "    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/integrations/spacy_plugin.py\", line 151, in __call__\n",
            "    self._parser.parse(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in parse\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in <listcomp>\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 193, in encode\n",
            "    encoded = self.retokenizer(example.words, example.space_after)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 150, in __call__\n",
            "    example = retokenize(self.tokenizer, words, space_after, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 76, in retokenize\n",
            "    assert word_idx == len(words) - 1\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Error in file: /content/drive/MyDrive/TFG/PRE_ELEC_SCRIPTS/Remarks_Following_the_Montana__South_Dakota__New_Mexico__New_Jersey_and_California_Primary_Elections.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 74, in retokenize\n",
            "    token_idx, (token_start, token_end) = next(offset_mapping_iter)\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "StopIteration\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-26-11fd102db1c0>\", line 48, in process_all_files\n",
            "    doc = nlp(text)\n",
            "          ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1057, in __call__\n",
            "    error_handler(name, proc, [doc], e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/util.py\", line 1722, in raise_error\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1052, in __call__\n",
            "    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/integrations/spacy_plugin.py\", line 151, in __call__\n",
            "    self._parser.parse(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in parse\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in <listcomp>\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 193, in encode\n",
            "    encoded = self.retokenizer(example.words, example.space_after)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 150, in __call__\n",
            "    example = retokenize(self.tokenizer, words, space_after, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 76, in retokenize\n",
            "    assert word_idx == len(words) - 1\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Error in file: /content/drive/MyDrive/TFG/PRE_ELEC_SCRIPTS/Remarks_at_Saint_Anselm_College_in_Manchester__New_Hampshire.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 74, in retokenize\n",
            "    token_idx, (token_start, token_end) = next(offset_mapping_iter)\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "StopIteration\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-26-11fd102db1c0>\", line 48, in process_all_files\n",
            "    doc = nlp(text)\n",
            "          ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1057, in __call__\n",
            "    error_handler(name, proc, [doc], e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/util.py\", line 1722, in raise_error\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1052, in __call__\n",
            "    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/integrations/spacy_plugin.py\", line 151, in __call__\n",
            "    self._parser.parse(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in parse\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in <listcomp>\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 193, in encode\n",
            "    encoded = self.retokenizer(example.words, example.space_after)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 150, in __call__\n",
            "    example = retokenize(self.tokenizer, words, space_after, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 76, in retokenize\n",
            "    assert word_idx == len(words) - 1\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Error in file: /content/drive/MyDrive/TFG/PRE_ELEC_SCRIPTS/Remarks_at_Trump_SoHo_in_New_York_City.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 74, in retokenize\n",
            "    token_idx, (token_start, token_end) = next(offset_mapping_iter)\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "StopIteration\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-26-11fd102db1c0>\", line 48, in process_all_files\n",
            "    doc = nlp(text)\n",
            "          ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1057, in __call__\n",
            "    error_handler(name, proc, [doc], e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/util.py\", line 1722, in raise_error\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1052, in __call__\n",
            "    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/integrations/spacy_plugin.py\", line 151, in __call__\n",
            "    self._parser.parse(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in parse\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in <listcomp>\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 193, in encode\n",
            "    encoded = self.retokenizer(example.words, example.space_after)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 150, in __call__\n",
            "    example = retokenize(self.tokenizer, words, space_after, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 76, in retokenize\n",
            "    assert word_idx == len(words) - 1\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Error in file: /content/drive/MyDrive/TFG/PRE_ELEC_SCRIPTS/Remarks_in_Virginia_Beach__Virginia.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 74, in retokenize\n",
            "    token_idx, (token_start, token_end) = next(offset_mapping_iter)\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "StopIteration\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-26-11fd102db1c0>\", line 48, in process_all_files\n",
            "    doc = nlp(text)\n",
            "          ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1057, in __call__\n",
            "    error_handler(name, proc, [doc], e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/util.py\", line 1722, in raise_error\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1052, in __call__\n",
            "    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/integrations/spacy_plugin.py\", line 151, in __call__\n",
            "    self._parser.parse(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in parse\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in <listcomp>\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 193, in encode\n",
            "    encoded = self.retokenizer(example.words, example.space_after)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 150, in __call__\n",
            "    example = retokenize(self.tokenizer, words, space_after, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 76, in retokenize\n",
            "    assert word_idx == len(words) - 1\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Error in file: /content/drive/MyDrive/TFG/PRE_ELEC_SCRIPTS/Remarks_Introducing_Governor_Mike_Pence_as_the_2016_Republican_Vice_Presidential_Nominee_in_New_York_City.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 74, in retokenize\n",
            "    token_idx, (token_start, token_end) = next(offset_mapping_iter)\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "StopIteration\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-26-11fd102db1c0>\", line 48, in process_all_files\n",
            "    doc = nlp(text)\n",
            "          ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1057, in __call__\n",
            "    error_handler(name, proc, [doc], e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/util.py\", line 1722, in raise_error\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1052, in __call__\n",
            "    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/integrations/spacy_plugin.py\", line 151, in __call__\n",
            "    self._parser.parse(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in parse\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in <listcomp>\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 193, in encode\n",
            "    encoded = self.retokenizer(example.words, example.space_after)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 150, in __call__\n",
            "    example = retokenize(self.tokenizer, words, space_after, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 76, in retokenize\n",
            "    assert word_idx == len(words) - 1\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Error in file: /content/drive/MyDrive/TFG/PRE_ELEC_SCRIPTS/Address_Accepting_the_Presidential_Nomination_at_the_Republican_National_Convention_in_Cleveland__Ohio.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 74, in retokenize\n",
            "    token_idx, (token_start, token_end) = next(offset_mapping_iter)\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "StopIteration\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-26-11fd102db1c0>\", line 48, in process_all_files\n",
            "    doc = nlp(text)\n",
            "          ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1057, in __call__\n",
            "    error_handler(name, proc, [doc], e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/util.py\", line 1722, in raise_error\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/language.py\", line 1052, in __call__\n",
            "    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/integrations/spacy_plugin.py\", line 151, in __call__\n",
            "    self._parser.parse(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in parse\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 414, in <listcomp>\n",
            "    encoded = [self.encode(example) for example in examples]\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py\", line 193, in encode\n",
            "    encoded = self.retokenizer(example.words, example.space_after)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 150, in __call__\n",
            "    example = retokenize(self.tokenizer, words, space_after, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/benepar/retokenization.py\", line 76, in retokenize\n",
            "    assert word_idx == len(words) - 1\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-bd4e4451db14>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"united states\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hillary clinton\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"america\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"our country\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"my opponent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"barack obama\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"our campaign\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"donald trump\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"your jobs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"our jobs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"illegal immigration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"immigration\"\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# customize as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_all_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-11fd102db1c0>\u001b[0m in \u001b[0;36mprocess_all_files\u001b[0;34m(file_list, keywords)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tree\n",
        "def tree_parser(doc):\n",
        "  for s in doc.sents:\n",
        "    print(s._.parse_string)\n",
        "\n",
        "  tree = Tree.fromstring(s._.parse_string)\n",
        "  tree.pretty_print()"
      ],
      "metadata": {
        "id": "QTdJSdKVknDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"so even though you say we want, as an example, general electric to produce more, if they don't want to or if for some reason one of the donors of crooked hillary clinton doesn't want that to happen, even though it's great for erie, even though it's great you, even though it's great for the state of pennsylvania, then it's not going to happen, folks.\"\n",
        "sdoc = nlp(sent) #tree_nlp\n",
        "\n",
        "tree_parser(sdoc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFq9ymy-n-Qp",
        "outputId": "b23c2d7b-f121-4300-d682-c5d5df240e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (RB so) (SBAR (SBAR (ADVP (RB even)) (IN though) (S (NP (PRP you)) (VP (VBP say) (SBAR (S (NP (PRP we)) (VP (VBP want) (, ,) (PP (IN as) (NP (DT an) (NN example))) (, ,) (NP (NP (JJ general) (NN electric)) (SBAR (S (VP (TO to) (VP (VB produce) (NP (JJR more))))))))))))) (, ,) (SBAR (SBAR (IN if) (S (NP (PRP they)) (VP (VBP do) (RB n't) (VP (VB want) (S (VP (TO to))))))) (CC or) (SBAR (IN if) (S (PP (IN for) (NP (DT some) (NN reason))) (NP (NP (CD one)) (PP (IN of) (NP (NP (DT the) (NNS donors)) (PP (IN of) (NP (JJ crooked) (JJ hillary) (NN clinton)))))) (VP (VBZ does) (RB n't) (VP (VB want) (S (NP (DT that)) (VP (TO to) (VP (VB happen)))))))))) (, ,) (SBAR (SBAR (SBAR (ADVP (RB even)) (IN though) (S (NP (PRP it)) (VP (VBZ 's) (ADJP (JJ great)) (PP (IN for) (NP (NN erie)))))) (, ,) (SBAR (SBAR (ADVP (RB even)) (IN though) (S (NP (PRP it)) (VP (VBZ 's) (ADJP (JJ great)) (PRP you)))) (, ,) (SBAR (ADVP (RB even)) (IN though) (S (NP (PRP it)) (VP (VBZ 's) (JJ great) (PP (IN for) (NP (NP (DT the) (NN state)) (PP (IN of) (NP (NN pennsylvania)))))))))) (, ,) (ADVP (RB then)) (NP (PRP it)) (VP (VBZ 's) (RB not) (VP (VBG going) (S (VP (TO to) (VP (VB happen) (, ,) (NP (NNS folks)))))))) (. .))\n",
            "                                                                                                                                                                                                                                         S                                                                                                                                                                                                                                              \n",
            "  _______________________________________________________________________________________________________________________________________________________________________________________________________________________________________|____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________   \n",
            " |                                                                                                                      SBAR                                                                                                                                          |                                                                                                 |                                                                                                             | \n",
            " |          _____________________________________________________________________________________________________________|_________________________________________________                                                                                           |                                                                                                 |                                                                                                             |  \n",
            " |        SBAR                                                                                          |                                                                  |                                                                                          |                                                                                                 |                                                                                                             | \n",
            " |    _____|_____________                                                                               |                                                                  |                                                                                          |                                                                                                 |                                                                                                             |  \n",
            " |   |     |             S                                                                              |                                                                  |                                                                                          |                                                                                                 |                                                                                                             | \n",
            " |   |     |      _______|________                                                                      |                                                                  |                                                                                          |                                                                                                 |                                                                                                             |  \n",
            " |   |     |     |                VP                                                                    |                                                                  |                                                                                          |                                                                                                SBAR                                                                                                           | \n",
            " |   |     |     |    ____________|_________________                                                    |                                                                  |                                                                                          |                                                                         ________________________|____________________________________________________________________________________                         |  \n",
            " |   |     |     |   |                             SBAR                                                 |                                                                  |                                                                                          |                                                                       SBAR                                                                                |   |    |                 |                        | \n",
            " |   |     |     |   |                              |                                                   |                                                                  |                                                                                          |          ______________________________________________________________|__________________                                                                |   |    |                 |                        |  \n",
            " |   |     |     |   |                              S                                                   |                                                                 SBAR                                                                                        |         |                                |                                               SBAR                                                             |   |    |                 |                        | \n",
            " |   |     |     |   |    __________________________|_____                                              |        __________________________________________________________|___________________                                                                       |         |                                |                ________________________________|_______________                                                |   |    |                 |                        |  \n",
            " |   |     |     |   |   |                                VP                                            |       |                          |                                                  SBAR                                                                    |         |                                |               |                      |                        SBAR                                             |   |    |                 |                        | \n",
            " |   |     |     |   |   |    ____________________________|________________________                     |       |                          |    _______________________________________________|______                                                                |         |                                |               |                      |    _____________________|____                                           |   |    |                 |                        |  \n",
            " |   |     |     |   |   |   |    |       |               |                        NP                   |      SBAR                        |   |                                                      S                                                               |         |                                |               |                      |   |     |                    S                                          |   |    |                 VP                       | \n",
            " |   |     |     |   |   |   |    |       |               |            ____________|_____               |    ___|____                      |   |        ______________________________________________|__________________________________                             |         |                                |               |                      |   |     |      ______________|___                                       |   |    |    _____________|____                    |  \n",
            " |   |     |     |   |   |   |    |       |               |           |                 SBAR            |   |        S                     |   |       |                   NP                                                            VP                           |        SBAR                              |               |                      |   |     |     |                  VP                                     |   |    |   |   |              VP                  | \n",
            " |   |     |     |   |   |   |    |       |               |           |                  |              |   |    ____|___                  |   |       |                ___|____                                                 ________|_________                   |    _____|_________                       |               |                      |   |     |     |     _____________|________                              |   |    |   |   |     _________|_____              |  \n",
            " |   |     |     |   |   |   |    |       |               |           |                  S              |   |   |        VP                |   |       |               |        PP                                              |    |             VP                 |   |     |         S                      |              SBAR                    |   |     |     |    |    |                 PP                            |   |    |   |   |    |               S             | \n",
            " |   |     |     |   |   |   |    |       |               |           |                  |              |   |   |     ___|_________        |   |       |               |    ____|______________                                 |    |    _________|___               |   |     |      ___|____                  |    ___________|________              |   |     |     |    |    |     ____________|____                         |   |    |   |   |    |               |             |  \n",
            " |   |     |     |   |   |   |    |       |               |           |                  VP             |   |   |    |   |         VP      |   |       |               |   |                   NP                               |    |   |             S              |   |     |     |        VP                |   |     |              S             |   |     |     |    |    |    |                 NP                       |   |    |   |   |    |               VP            | \n",
            " |   |     |     |   |   |   |    |       |               |           |             _____|_____         |   |   |    |   |     ____|___    |   |       |               |   |         __________|____________                    |    |   |     ________|___           |   |     |     |    ____|________         |   |     |      ________|____         |   |     |     |    |    |    |        _________|_______                 |   |    |   |   |    |     __________|___          |  \n",
            " |   |     |     |   |   |   |    |       PP              |           |            |           VP       |   |   |    |   |    |        S   |   |       PP              |   |        |                       PP                  |    |   |    |            VP         |   |     |     |   |    |        PP       |   |     |     |             VP       |   |     |     |    |    |    |       |                 PP               |   |    |   |   |    |    |              VP        | \n",
            " |   |     |     |   |   |   |    |    ___|___            |           |            |      _____|___     |   |   |    |   |    |        |   |   |    ___|____           |   |        |           ____________|_____              |    |   |    |         ___|____      |   |     |     |   |    |     ___|___     |   |     |     |     ________|____    |   |     |     |    |    |    |       |              ___|_______         |   |    |   |   |    |    |     _________|____     |  \n",
            " |  ADVP   |     NP  |   NP  |    |   |       NP          |           NP           |     |         NP   |   |   NP   |   |    |        VP  |   |   |        NP         NP  |        NP         |                  NP            |    |   |    NP       |        VP    |  ADVP   |     NP  |   ADJP  |       NP   |  ADVP   |     NP   |       ADJP  |   |  ADVP   |     NP   |    |    |       NP            |           NP       |  ADVP  NP  |   |    |    |    |         |    NP   | \n",
            " |   |     |     |   |   |   |    |   |    ___|_____      |      _____|_____       |     |         |    |   |   |    |   |    |        |   |   |   |    ____|____      |   |     ___|____      |       ___________|_______      |    |   |    |        |        |     |   |     |     |   |    |    |       |    |   |     |     |    |        |    |   |   |     |     |    |    |    |    ___|____         |           |        |   |    |   |   |    |    |    |         |    |    |  \n",
            " RB  RB    IN   PRP VBP PRP VBP   ,   IN  DT        NN    ,     JJ          NN     TO    VB       JJR   ,   IN PRP  VBP  RB   VB       TO  CC  IN  IN  DT        NN    CD  IN   DT      NNS    IN     JJ          JJ      NN   VBZ   RB  VB   DT       TO       VB    ,   RB    IN   PRP VBZ   JJ   IN      NN   ,   RB    IN   PRP  VBZ       JJ  PRP  ,   RB    IN   PRP  VBZ   JJ   IN  DT       NN       IN          NN       ,   RB  PRP VBZ  RB  VBG   TO   VB        ,   NNS   . \n",
            " |   |     |     |   |   |   |    |   |   |         |     |     |           |      |     |         |    |   |   |    |   |    |        |   |   |   |   |         |     |   |    |        |     |      |           |       |     |    |   |    |        |        |     |   |     |     |   |    |    |       |    |   |     |     |    |        |    |   |   |     |     |    |    |    |   |        |        |           |        |   |    |   |   |    |    |    |         |    |    |  \n",
            " so even though you say  we want  ,   as  an     example  ,  general     electric  to produce     more  ,   if they  do n't  want      to  or  if for some     reason one  of  the     donors  of  crooked     hillary clinton does n't want that      to     happen  ,  even though  it  's great for     erie  ,  even though  it   's     great you  ,  even though  it   's great for the     state      of     pennsylvania  ,  then  it  's not going  to happen      ,  folks  . \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IOKyNgrk6oRT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}